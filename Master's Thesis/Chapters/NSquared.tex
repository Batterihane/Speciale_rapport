\chapter{\todo{The NSquared algorithm}}
Goddard et. al.\cite{nsquared} describes a $O(n^2)$ algorithm for finding the maximum agreement subtree for two rooted binary trees. Given two trees T and U of size m and n, the idea is to iteratively find the largest agreement subtree and its size for every pair of subtrees from T and U. This can be done in quadratic time by using Lemma 1. 

\begin{Lemma}
	Let $T_a$ be a tree rooted at vertex a with the children b and c; And let $T_w$ be rooted at w with children x and y. We now define $\#(T_a,T_w)$ as the size of the maximum agreement subtree of $T_a$ and $T_w$.  $\#(T_a,T_w)$ is given by the maximum of the following six numbers: \#$(T_b,T_x)+(T_c,T_y)$,
	\#$(T_b,T_y)+(T_c,T_x)$,
	\#$(T_a,T_x)$,
	\#$(T_a,T_y)$,
	\#$(T_b,T_w)$,
	\#$(T_c,T_w)$.\\
	\textbf{Proof.}\\
	This is a proof by case over the possible subtrees contributing to the agreement subtree for $T_a$ and $T_w$, $A_{aw}$.\\
	\underline{Case 1}: Only $T_b$ of $T_a$ contributes to $A_{aw}$.
	If this is the case, then the agreement subtree can only exist between $T_b$ and $T_w$, \#$(T_b,T_w)$.\\
	\underline{Case 2}: Only $T_c$ of $T_a$ contributes to $A_{aw}$.
	If this is the case, then the agreement subtree can only exist between $T_c$ and $T_w$,
	\#$(T_c,T_w)$.\\
	\underline{Case 3}: Only $T_x$ of $T_w$ contributes to $A_{aw}$.
	If this is the case, then the agreement subtree can only exist between $T_x$ and $T_a$,
	\#$(T_a,T_x)$.\\
	\underline{Case 4}: Only $T_y$ of $T_w$ contributes to $A_{aw}$.
	If this is the case, then the agreement subtree can only exist between $T_y$ and $T_a$,
	\#$(T_a,T_y)$.\\
	\underline{Case 5}: This case covers all that case 1 through 4 doesn't, which means that all children b, c, x and y contibute to the MAST.
	Consider the agreement tree, $A_r$, rooted in r and with children j and l. We claim that a child of r cannot contain a leaf, $b'$, from $T_b$ and a leaf, $c'$, from $T_c$ at the same time. If this is the case, then we only need to find the MAST size of child pairing permutations across $T_a\ and\ T_w$: $max\{\#(T_b, T_x)+\#(T_c, T_y), \#(T_b, T_y)+\#(T_c, T_x)\}$.
	Let us now prove our claim to be correct by assuming it doesn't hold, and use that to reach a  contradiction. Suppose that $A_j$ contains a leaf , $b'$, from $T_b$ and a leaf, $c'$, from $T_c$. Let f be another leaf from $T_b$. We now see that the subtree induced by $b', f and c'$ differs for $A_r$ and $T_a$ as illustrated in Figure \ref{Fig:Lemma1Binary}. This is clearly contradictory to the definition of an agreement tree, which is why we must conclude our claim to be correct.
	\qed   	  
\end{Lemma}

\begin{figure}
	\begin{enumerate}
		
		\item[]  \Tree [.r [.j b' c' ].j [.l f ].l ].r
		\hskip 0.4in
		 \Tree [.a [.b b' f ].b [.c c' ].c ].a
		
	\end{enumerate}	
	
	\caption{Two example binary trees for, and they resulting MAST}
	\label{Fig:Lemma1Binary}	
\end{figure}


What Lemma 1 essentially states is twofold. First, if the MAST contains children from all subtrees (a,b,x,y), then its size is given by the largest combination of these. Secondly, if the MAST does not include all subtrees, then its size is given by the largest combination of subtrees excluding at least one of them.    
This gives rise to a recursive solution for finding the size of the maximum agreement subtree. The recursive function is defined in figure \ref{Fig:Function1}.

\begin{figure}
	\begin{equation*}
	\begin{aligned}
	f(T_a,T_w)=Max
	\begin{cases}
	f(T_b,T_x)+f(T_c,T_y) & \text{if Type($T_a$)=Type($T_w$)=Internal Node}
	\\
	f(T_b,T_y)+f(T_c,T_x) & \text{if Type($T_a$)=Type($T_w$)=Internal Node}
	\\
	f(T_a, T_x)           & \text{if Type($T_w$)=Internal Node}
	\\
	f(T_a, T_y)           & \text{if Type($T_w$)=Internal Node}
	\\
	f(T_b, T_w)           & \text{if Type($T_a$)=Internal Node}
	\\
	f(T_c, T_w)           & \text{if Type($T_a$)=Internal Node}
	\\
	1 	                  & \text{if Type($T_a$)=Type($T_w$)=Leaf  $\land$  $T_a$=$T_w$}
	\\
	0                     
	\end{cases}
	\end{aligned}
	\phantom{\hspace{6cm}}
	\end{equation*}
	\caption{Recursive MAST size function}
	\label{Fig:Function1}
\end{figure}


Like many similar recursive definitions in bioinformatics, we run into the problem that the recursive function computes the same partial results multiple times. This problem is rectified by the method of dynamic programming. Specifically, we wish to store the partial results in a $|T_a| \times |T_w|$ table. We seek to fill out the table with our partial results, and end up with the maximum size in the bottom right corner.
Looking at the function in Figure \ref{Fig:Function1} we see that the partial result for each tree node is dependent on the results of its children. This implies that we must list the table nodes in postorder, thereby ensuring that the partial results, on which a given node is dependent, has already been calculated when we reach it. 
By introducing such a table we also introduce a requirement of $O(n^2)$ space and time, making it a quadratic algorithm. Simple techniques for reducing the space requirement, like only storing a few number of rows in the table at a time, cannot be applied in this case, since the computation of a new row may depend on any of the previous rows.  
\\
We will in the following sections show how one can extend this method for calculation the size of the MAST to computing the actual tree.

\subsection{Algorithm example}
Two example trees have been provided in Figure \ref{Fig:Binary1}. We now wish to compute the size of their maximum agreement subtree using the agorithm described above. For this reason, we create a $15 \times 15$ table containing the nodes of each tree in postorder. Using the function described in \ref{Fig:Function1}, we can now fill out the table row wise by starting from the topmost row. The resulting table can be seen in \ref{Table:Table1}, where we from the bottom right corner than see that the maximum size of the MAST is 6. By looking at the trees for a little while, one can come to the conclusion that 6 is indeed correct, since removing only one leaf cannot produce an agreement tree in this case, while removing $leaf_1$ and $leaf_7$ will produce the desired agreement tree of size 6. Their MAST is likewise displayed in Figure \ref{Fig:Binary1}. 

\begin{figure}
	
	\begin{itemize}
		\setlength\itemsep{3em}
		\item[] \Tree [.A [.B [.C leaf_1 leaf_2 ] [.D leaf_3 leaf_4 ] ].B [.E [.F leaf_5 leaf_6 ] [.G leaf_7 leaf_8 ] ].E ].A
		
		\item[]
		\Tree [.H [.I [.J leaf_1  leaf_8 ] [.L leaf_5 leaf_6 ] ].I [.M [.N leaf_2 leaf_7 ] [.F leaf_3 leaf_4 ] ].M ].H
		
		\item[]
		\Tree [.A [.B leaf_2 [.D leaf_3 leaf_4 ] ].B [.E [.F leaf_5 leaf_6 ] leaf_8 ].E ].A
	\end{itemize}	
	
	\caption{Two example binary trees for, and they resulting MAST}
	\label{Fig:Binary1}	
\end{figure}


\begin{table}[]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{}        & $\mathbf{leaf_1}$ & $\mathbf{leaf_2}$ & \textbf{C} & \textbf{...} & \textbf{G} & \textbf{E} & \textbf{A} \\ \hline
		$\mathbf{leaf_1}$ & 1                & 0                & 1          & ...            & 0          & 0          & 1          \\ \hline
		$\mathbf{leaf_8}$ & 0                & 0                & 0          & ...            & 1          & 1          & 1          \\ \hline
		\textbf{J}       & 1                & 0                & 1          & ...            & 1          & 1          & 2          \\ \hline
		\textbf{...}     & ...              & ...              & ...        & ...            & ...        & ...        & ...        \\ \hline
		\textbf{F}       & 0                & 0                & 0          & ...            & 0          & 0          & 2          \\ \hline
		\textbf{M}       & 0                & 1                & 1          & ...            & 1          & 1          & 3          \\ \hline
		\textbf{H}       & 1                & 1                & 2          & ...            & 2          & 3          & 6          \\ \hline
	\end{tabular}
	
	\caption{Score Matrix for the Trees in Figure \ref{Fig:Binary1}}
	\label{Table:Table1}
\end{table}




\subsection{Computing the MAST}
Extending the solution for computing the size of the MAST to include the computation of the actual MAST is not covered in the article \cite{nsquared}, but is fairly simple in this case. We found two different ways of going about it. 
\\
\\
\textbf{Option 1: Continual Tree Construction} \\
The first option is to extend the size algorithm simply by attaching the MAST responsible for the size of each cell in the size table. This cannot be done by copying subtrees from the input trees, since that would break our time constraints. For this reason, we store the pointer to the root of the subtree for each cell, and hence mutiple cells/masts will refer to the same subtrees. This means that we will end up creating a Directed Acyclic Graph, where the root node of the maximum sized MAST is saved in the bottom right corner of the size table. Usually a traversal fixing parent/child pointers is needed in the end.
More concretely, the algorithm is described as follows: 

For each pair of nodes $a$ and $w$ we compute the agreement subtree $A_{a,w}$ for the two subtrees $T_a$ and $T_w$ having respectively $a$ and $w$ as roots. For such a pair of nodes there are three cases.

The first case is that $a$ and $w$ are both leaves. If the leaves are equal, i.e. they have the same name, then $A_{a,w}$ will be the tree consisting of exactly one leaf with that name. Otherwise $A_{a,w}$ is empty.

The second case is that we have a leaf and an internal node. Let's say $w$ is the internal node having $x$ and $y$ as children. If the leaf corresponding to $a$ is contained in $T_w$, it will also be contained in either $T_x$ or $T_y$ and $A_{a,w}$ will be the same as either $A_{a,x}$ or $A_{a,y}$. Since we do a postorder traversal of the trees, $A_{a,x}$ and $A_{a,y}$ have already been computed and $A_{a,w}$ can just be set to the largest of the two.

The third case is that both $a$ and $w$ are internal nodes, where $a$ have children $b$ and $c$ and $w$ have children $x$ and $y$. In this case we can use Lemma 1 and determine which of the six cases gives the largest size. Again, all the agreement subtrees to consider have already been computed. If the largest size is $\#A_{b,x} + \#A_{c,y}$, then $A_{a,w}$ will be a tree where the root has the roots of $A_{b,x}$ and $A_{c,y}$ as children. Similarly if the largest size is $\#A_{b,y} + \#A_{c,x}$.\\
\\
\textbf{Option 2: Recursive Backtracking} \\
While the first option is fairly simple to implement and understand, it can consume quite a bit of memory due to all the trees we save that ultimately end up being irrelevant for the final result. Backtracking tries to rectify this problem by only computing and storing exactly the relevant subtrees for the final MAST. This is done by observing that the formula in Figure \ref{Fig:Function1}, on which the algorithm is based, is a maximization function. Therefore, the table is deterministicly constructed, allowing us for each cell to determine what cells were responsible for the size it contains. By computing the entire size table first, we can start with the bottom right cell and recursively run back through the table determining all cells responsible for the MAST size. The tree is then constructed following the same logic used in Option 1. 
\\
Java inspired pseudo-code for the backtracking algorithm can be seen in Figure \ref{Code:Backtracking1}. Notice that the order of the if-cases in the pseudo code are significant, which was not previously the case when only computing the MAST size. The reason is simply that several of the cases might actually lead to the same MAST size, but not all will lead to a valid binary tree. For instance the second and third recursive case might lead to the same size, but if you choose the third case, then you will end up will a null-branch in the produced MAST, since this branch does not contribute to the MAST size. 
    
\begin{figure}
	\begin{lstlisting}[language=Java]
	Function RecBackTrack(Tree1, Tree2)	{
	  int Score = ScoreMatrix[Tree1][Tree2];
	  
	  /* Base Case */
	  if (Type(Tree1) = Type(Tree2) = Leaf) {
	    return Tree1 = Tree2 ? Tree1 : null; 
	  }
	  
	  /* Recursive Case 1 */
	  if (Type(Tree1) = InternalNode) {
	    int Score1 = ScoreMatrix[Tree1.Child1][Tree2]; 
	    int Score2 = ScoreMatrix[Tree1.Child2][Tree2];
	  
	    if (Score = Score1) {
	      return RecBackTrack(Tree1.Child1, Tree2);
	    }
	    else if (Score = Score2) {
	      return RecBackTrack(Tree1.Child2, Tree2);
	    }
	  }
	  
	  /* Recursive Case 2 */
	  if (Type(Tree2) = InternalNode) {
	    int Score1 = ScoreMatrix[Tree1][Tree2.Child1]; 
	    int Score2 = ScoreMatrix[Tree1][Tree2.Child2];
	  
	    if (Score = Score1) {
	      return RecBackTrack(Tree1, Tree2.Child1);
	    }
	    else if (Score = Score2) {
	      return RecBackTrack(Tree1, Tree2.Child2);
	    }
	  }	
	  
	  /* Recursive Case 3 */
	  if(Type(Tree1) = Type(Tree2) = InternalNode) {
	    int Score1 = ScoreMatrix[Tree1.Child1][Tree2.Child1];
	    int Score2 = ScoreMatrix[Tree1.Child2][Tree2.Child2];
	    int Score3 = ScoreMatrix[Tree1.Child1][Tree2.Child2];
	    int Score4 = ScoreMatrix[Tree1.Child2][Tree2.Child1];
	
	    if(Score = Score1 + Score2) {
	      Tree subTree1 = RecBackTrack(Tree1.Child1, Tree2.Child1);
	      Tree subTree2 = RecBackTrack(Tree1.Child2, Tree2.Child2);
	      return /*internal node with subTree1 and 
	        subTree2 as children */
	    }
	    else if (Score1 = Score3 + Score4) {
	      Tree subTree1 = RecBackTrack(Tree1.Child1, Tree2.Child2);
          Tree subTree2 = RecBackTrack(Tree1.Child2, Tree2.Child1);
	      return /*internal node with subTree1 and 
	        subTree2 as children */
	    }
	  }
	}
	\end{lstlisting}
	\caption{Java pseudo code for BackTracking}
	\label{Code:Backtracking1}
\end{figure}	

\section{Evaluation}
All in all, the $O(n^2)$ algorithm is fairly simple to implement and understand, but it also has its practical limitations when working with large trees. As we will see in the testing section, the algorithm starts having trouble with binary trees of size 6000 in terms of time and memory consumption. The main problem with the algorithm is that it considers too many combinations of subtrees that end up being irrelevant. Effectively most of the time spent filling out the size table is wasted. In our previous example of filling out the size table of 225 cells, only 25 cells ended up ultimately having an impact on the produced MAST, since those were the only ones accesed by the backtracking algorithm. The $O(nlogn)$ algorithm that we are about to examine improves on the 'wasted' work by only considering more relevant information between subtrees.
Similarly to the naive algorithm, there are no worst-case or best-case trees to consider, since we always fill out the entire size table despite what the tree topology might look like. 
